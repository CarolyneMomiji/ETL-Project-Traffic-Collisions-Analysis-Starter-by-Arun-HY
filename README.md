This assignment aims to provide hands-on experience in analysing large-scale traffic collision datasets using PySpark and AWS services. You will apply data analytics techniques to clean, transform and explore crash data, drawing meaningful insights to support traffic safety and urban planning. Beyond understanding how big data tools optimise performance on a single machine and across clusters, you will develop a structured approach to analysing crash trends, identifying high-risk locations and evaluating contributing factors to traffic incidents. Additionally, you will utilise AWS S3 to store the processed data efficiently after the ETL process, enabling scalable storage and easy retrieval for further analysis.  

Business Value
Traffic collisions pose significant risks to public safety, requiring continuous monitoring and analysis to enhance road safety measures. Government agencies, city planners and policymakers must leverage data-driven insights to improve infrastructure, optimise traffic management and implement preventive measures. In this assignment, you will analyse California traffic collision data to uncover patterns related to accident severity, location-based risks and key contributing factors. With Apache Spark’s ability to handle large datasets efficiently and AWS S3’s scalable storage, transportation authorities can process vast amounts of crash data in real time, enabling faster and more informed decision-making.


As an analyst examining traffic safety trends, your task will be to analyse historical crash data to derive actionable insights that can drive policy improvements and safety interventions. Your analysis will help identify high-risk areas, categorise accidents by severity and contributing factors and store the processed data in an AWS S3 bucket for scalable and long-term storage. By leveraging big data analytics and cloud-based storage, urban planners and traffic authorities can enhance road safety strategies, reduce accident rates and improve public transportation planning.

 
